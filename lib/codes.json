{"pig": "```\n#!/usr/bin/python\nimport os, sys\n\nfrom org.apache.pig.scripting import *\n\nif len(sys.argv) != 2:\n    print 'Usage: pig -useHCatalog %s <date>' % (sys.argv[0])\n\ndates = sys.argv[1].split(',')\n\nfor date in dates:\n    params = {\n        'default_parallel': '20',\n        'job_name': 'nearby_clickinfo_%s' % date,\n        'data_type': 'daily_data',  # daily_data or hourly_data\n        'spaceid': '958806385',\n        'date': date,\n        'proj_root': 'nearby_clickinfo/%s' % date\n    }\n\n    # Clean up destination\n    Pig.fs('rm -r -skipTrash %s' % params['proj_root'])\n\n    # Init Pig object\n    # https://pig.apache.org/docs/r0.15.0/api/org/apache/pig/scripting/Pig.html\n    P = Pig.compile('''\n    set mapreduce.reduce.speculative true\n    set mapreduce.map.speculative true\n    set default_parallel '$default_parallel'\n    set job.name '$job_name'\n\n    benzene = load 'benzene.$data_type' using org.apache.hive.hcatalog.pig.HCatLoader();\n\n    clicks = filter benzene by dt == '$date'\n      and network == 'on'\n      and pty_family == 'search'\n      and pty_experience == 'web'\n      and pty_country == 'tw'\n      and spaceid == '$spaceid'\n      and event_family == 'interaction'\n      and click_info is not null\n      and click_info#'t2' == 'appNearby';\n\n    clicks = foreach clicks generate\n        logged_event_timestamp as timestamp,\n        spaceid,\n        page_uri as uri,\n        click_info;\n\n    store clicks into '$proj_root';\n    ''')\n\n\n\n    # Get BoundScript object before running it\n    # https://pig.apache.org/docs/r0.15.0/api/org/apache/pig/scripting/BoundScript.html\n    bound = P.bind(params)\n\n    stats = bound.runSingle()\n\n    if not stats.isSuccessful():\n        raise 'Pig job failed'\n\n    # How to fetch HDFS file\n    # hadoop fs -get / -getmerge hdfs_dir\n    Pig.fs('getmerge %s %s' % ( params['proj_root'], os.path.join('files', str(date)) ))\n    #Pig.fs('getmerge %s %s' % [ params['proj_root'], sys.argv[0] ])\n```"}